<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Muhammad Monjurul Karim</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />


        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Muhammad Monjurul Karim</span>
<!--                 <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span> -->
                <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto mb-2" src="assets/img/website.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Teaching">Teaching</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view" target="_blank">CV</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#blog">blog</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="demo/gallery.html">Gallery</a></li>
                    
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#gallery">Gallery</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Muhammad Monjurul
                        <span class="text-primary">Karim</span>
                    </h1>
                    <div class="subheading mb-5">
                        112 More Hall · University of Washington · Seattle · WA 98195 · (609) 787-9233 ·
                        <a href="mailto:mmkarim@uw.edu">mmkarim@uw.edu</a>
                    </div>
<!--                     <p class="lead mb-1">I am a Ph.D. student of Civil Engineering at the Stony Brook University working with <a href="https://sites.google.com/stonybrook.edu/rqin/home"> Dr. Ruwen Qin</a> and <a href="https://www.cs.stonybrook.edu/people/faculty/ZhaozhengYin">Dr. Zhaozheng Yin</a>.</p> -->
                        <p class="lead mb-1">I am a Postdoctoral Scholar at the Smart Transportation Applications and Research Lab <a href="http://www.uwstarlab.org/people.html"> (STAR Lab) </a> at the University of Washington in Seattle. I earned my Ph.D. in Civil Engineering from Stony Brook University and an M.S. in Systems Engineering from the Missouri University of Science and Technology.
                
                    <p class="lead mb-5">
                     My research goal is to provide visual perception to the real world applications by achieving artificial intelligence. I am interested in solving large-scale visual recognition and prediction problems by developing novel deep learning approaches. My research experience includes but not limited to traffic accident anticipation, risk localization, object detection/segmentation, tracking, visual attention, and remote sensing.</p>

                    <p class="lead mb-5"> Please, find my <a href="https://bit.ly/3fSi38M"> CV</a>.</p> 
<!--                     <a href="http://bit.ly/3JegD8a"> Resume </a> -->

                    <div class="social-icons">
<!--                         <a class="social-icon" href="https://www.linkedin.com/in/muhammad-monjurul-karim-18726279/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/monjurulkarim/"><i class="fab fa-github"></i></a> -->
                        <a href="https://www.linkedin.com/in/muhammad-monjurul-karim-18726279/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <a href="http://bit.ly/3IUS26Z"><img src="assets\img\github_square.png" height="44px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <a href="http://bit.ly/3ygbtC0"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <!-- <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="https://www.facebook.com/monjurulkarimraju/"><i class="fab fa-facebook-f"></i></a> -->
                    </div>
                </div>
            </section>
            <hr class="m-0" />


            <!-- Education-->
            <!-- <section class="resume-section" id="education"> -->
                <!-- <div class="resume-section-content"> -->
                    <!-- <h2 class="mb-5">Education</h2> -->
                    <!-- <div class="d-flex flex-column flex-md-row justify-content-between mb-5"> -->
                        <!-- <div class="flex-grow-1"> -->
                            <!-- <h3 class="mb-0">Stony Brook University</h3> -->
                            <!-- <div class="subheading mb-3">Ph.D. Student</div> -->
                            <!-- <div>Civil Engineering</div> -->
                            <!-- <p>GPA: 4.00</p> -->
                        <!-- </div> -->
                        <!-- <div class="flex-shrink-0"><span class="text-primary">August 2020 - Present</span></div> -->
                    <!-- </div> -->
                    <!-- <div class="d-flex flex-column flex-md-row justify-content-between mb-5"> -->
                        <!-- <div class="flex-grow-1"> -->
                            <!-- <h3 class="mb-0">Missouri University of Science and Technology</h3> -->
                            <!-- <div class="subheading mb-3">Masters of Science</div> -->
                            <!-- <div>Systems Engineering</div> -->
                            <!-- <div>Thesis: Computer vision based deep learning models for cyber physical systems. |<a href="https://scholarsmine.mst.edu/masters_theses/7955/"> Download</a></div> -->
                            <!-- <p>GPA: 4.00/4.00</p> -->
                        <!-- </div> -->
                        <!-- <div class="flex-shrink-0"><span class="text-primary">August 2018 - July 2020</span></div> -->
                    <!-- </div> -->
                    <!-- <div class="d-flex flex-column flex-md-row justify-content-between"> -->
                        <!-- <div class="flex-grow-1"> -->
                            <!-- <h3 class="mb-0">Bangladesh University of Engineering and Technology</h3> -->
                            <!-- <div class="subheading mb-3">Bachelor of Science</div> -->
                            <!-- <div>Industrial and Production Engineering</div> -->
                            <!-- <p>GPA: 3.58/4.00</p> -->
                        <!-- </div> -->
                        <!-- <div class="flex-shrink-0"><span class="text-primary">August 2009 - June 2014</span></div> -->
                    <!-- </div> -->
                <!-- </div> -->
            <!-- </section> -->
            <!-- <hr class="m-0" /> -->

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                          <h3 class="mb-0">Edge-AI Powered Real-Time Traffic Conflict Estimation at Urban Intersections</h3>
                          <div class="subheading mb-3"></div>
                          <!-- <video src="assets\img\Media1.mp4" alt="risk-localization" width=width="760" height="480"> -->
                            <video width="760" height="480" controls autoplay loop muted>
                              <source src="assets\img\Live_detection_UI.mp4" type="video/mp4">
                                </video>
                          <div class="subheading mb-3"></div>
                          <p> <b>Abstract:</b> Traffic intersection safety is a growing concern in urbanizing areas with dense traffic and complex road-user interactions. Real-time conflict estimation is essential for reducing risks, yet existing cloud-based methods face challenges like high transmission costs, impractical data handling, and privacy issues due to centralized storage.

                            To address these challenges, we propose an Edge-AI-based conflict estimation system for busy intersections. By processing data locally on edge devices, our approach reduces bandwidth usage, lowers reliance on expensive cloud infrastructure, minimizes centralized server loads, and enhances privacy by avoiding raw data storage.
                            
                            The system utilizes a roadside camera with an edge device running a lightweight, modified YOLOv8 model for real-time detection and the ByteTrack algorithm for object tracking. Camera calibration converts image coordinates into real-world coordinates, enabling the estimation of vehicle speed, heading, and future trajectory. Using this data, the system predicts potential conflicts by calculating speed, heading, distance, and time-to-collision (TTC) in real time, ensuring timely interventions with low latency.
                            <a href='https://github.com/monjurulkarim/YOLOv8_Object_Tracking_TensorRT'><b> Project Link </b></a>
                              <!-- <p> <h5> Project code will be uploaded very soon. </h5></p> -->
<!--                                 <a href='https://github.com/monjurulkarim/DSTA'><b> Project Link </b></a> -->
                          </p>
                          <div class="subsubheading mb-0"><b> This resarch is supported by Leidos Inc.</b></div>
                      </div>
                      <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                  </div>
  <hr>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                                <div class="flex-grow-1">
                                    <h3 class="mb-0">Risky object localization in driving videos</h3>
                                    <div class="subheading mb-3"></div>
                                    <!-- <video src="assets\img\Media1.mp4" alt="risk-localization" width=width="760" height="480"> -->
                                      <video width="760" height="480" controls autoplay loop muted>
                                        <source src="assets\img\Media1.mp4" type="video/mp4">
                                          </video>
                                    <div class="subheading mb-3"></div>
                                    <p> <b>Abstract:</b> Detecting dangerous traffic agents in videos captured by vehicle-mounted dashboard cameras (dashcams) is essential to facilitate safe navigation in a complex environment. Accident-related videos are just a minor portion of the driving video big data, and the transient pre-accident processes are highly dynamic and complex. Besides, risky and non-risky traffic agents can be similar in their appearance. These make risky object localization in the driving video particularly challenging. To this end, we developed an attention-guided multistream feature fusion network (AM-Net) to localize dangerous traffic agents from dashcam videos. Two Gated Recurrent Unit (GRU) networks use object bounding box and optical flow features extracted from consecutive video frames to capture spatio-temporal cues for distinguishing dangerous traffic agents. An attention module coupled with the GRUs learns to attend to the traffic agents relevant to an accident. Fusing the two streams of features, AM-Net predicts the riskiness scores of traffic agents in the video. In supporting this study, we also introduced a benchmark dataset called Risky Object Localization (ROL).
                                      <a href='https://github.com/monjurulkarim/ROL_Dataset'><b> Project Link </b></a>
                                        <!-- <p> <h5> Project code will be uploaded very soon. </h5></p> -->
        <!--                                 <a href='https://github.com/monjurulkarim/DSTA'><b> Project Link </b></a> -->
                                    </p>
                                    <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                                </div>
                                <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                            </div>
            <hr>



                        <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Predicting future traffic accidents with vehicle mounted camera</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\project3.PNG" alt="accident" width="512" height="640">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> The rapid advancement of sensor technologies and artificial intelligence are creating new opportunities for traffic safety enhancement. Dashboard cameras (dashcams) have been widely deployed on both human driving vehicles and automated driving vehicles. A computational intelligence model that can accurately and promptly predict accidents from the dashcam video will improve the preparedness for accident prevention. The spatial-temporal interaction of traffic agents is complex. Visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, the early anticipation of traffic accidents remains a challenge. Inspired by the humans’ attention behavior in visually perceiving accident risks, this paper proposes a Dynamic Spatial-Temporal Attention (DSTA) network for the early accident anticipation from dashcam videos. The DSTA-network learns to select discriminative temporal segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It also learns to focus on the informative spatial regions of frames with a Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) network is trained jointly with the attention modules to predict the probability of a future accident.
<!--                                 <p> <h5> Article of this project is under review. Project code will be uploaded after getting the paper acceptance decision. </h5></p> -->
<!--                                 <a href='https://github.com/monjurulkarim/DSTA'><b> Project Link </b></a> -->
                            </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
            <hr>


            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Explainable Artificial Intelligence for Traffic Accident Prediction</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\grad_cam.PNG" alt="grad-cam" width=width="760" height="480">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> Traffic accident anticipation is a vital function of Automated Driving Systems (ADSs) for providing a safety-guaranteed driving experience. An accident anticipation model aims to predict accidents promptly and accurately before they occur. Existing Artificial Intelligence (AI) models of accident anticipation lack a human-interpretable explanation of their decision-making. Although these models perform well, they remain a black-box to the ADS users, thus difficult to get their trust. To this end, this paper presents a Gated Recurrent Unit (GRU) network that learns spatio-temporal relational features for the early anticipation of traffic accidents from dashcam video data. A post-hoc attention mechanism named Grad-CAM is integrated into the network to generate saliency maps as the visual explanation of the accident anticipation decision. An eye tracker captures human eye fixation points for generating human attention maps. The explainability of network-generated saliency maps is evaluated in comparison to human attention maps.
                                <!-- <p> <h5> Project code will be uploaded very soon. </h5></p> -->
<!--                                 <a href='https://github.com/monjurulkarim/DSTA'><b> Project Link </b></a> -->
                            </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">

                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bridge Inspection Video Data Analysis for Data-driven Asset Management</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\fig_1_overview.png" alt="Overview" width="720" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> Inspection  of  the  transportation  infrastructure,  such  as  bridges,  is  an  important  step  towards  the  preservation  andrehabilitation of the infrastructure for extending their service lives. The advancement of mobile robotic technology hasmade it possible to rapidly collect a large amount of inspection video data. Yet, the data are mainly images of complexscenes, wherein a bridge of various structural elements mix with a cluttered background. Assisting bridge inspectors inextracting structural elements of bridges from the big complex video data, and sorting them out by classes, will prepareinspectors for the element-wise inspection to determine the condition of bridges. This paper is motivated to developan assistive intelligence model for segmenting multiclass bridge elements from inspection videos captured by an aerialinspection platform. First, with a small initial training dataset labeled by inspectors, a Mask Region-based ConvolutionalNeural Network (Mask R-CNN) pre-trained on a large public dataset was transferred to the new task of multiclass bridgeelement segmentation. Then, the temporal coherence analysis attempts to recover false negative detections by thetransferred network. Finally, a semi-supervised self-training (S3T) algorithm was developed, which leverages inspectors’domain knowledge into the intelligence model by engaging them in refining the network iteratively.
    <div class="row">
  <div class="column">
    <img src="assets\img\bridge_inspect1.gif" alt="Snow" style="width:100%">
  </div>
  <div class="column">
    <img src="assets\img\bridge_inspect2.gif" alt="Forest" style="width:100%">
  </div>
                            </div>

                                <a href='https://github.com/monjurulkarim/active_learning'><b> Project Link </b></a>  </p>
<!--                             <div class="subsubheading mb-0"><a href='https://github.com/monjurulkarim/active_learning'> Project Link</a> </div> -->
                            <div class="subsubheading mb-0"> <b>This resarch is supported by INSPIRE University Transportation Center <a href='http://inspire-utc.mst.edu'> (http://inspire-utc.mst.edu) </a>. </b> </div>

                        </div>

                        <!-- <div class="flex-shrink-0"><span class="text-primary">March 2013 - Present</span></div> -->
                    </div>


                 <hr>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\project2.PNG" alt="multinet" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> To assist human drivers and autonomous vehicles in assessing crash risks, driving scene analysis using dash cameras on vehicles and deep learning algorithms is of paramount importance. Although these technologies are increasingly available, driving scene analysis for this purpose still remains a challenge. This is mainly due to the lack of annotated large image datasets for analyzing crash risk indicators and crash likelihood, and the lack of an effective method to extract lots of required information from complex driving scenes. To fill the gap, this paper develops a scene analysis system. The Multi-Net of the system includes two multi-task neural networks that perform scene classification to provide four labels for each scene. The DeepLab v3 and YOLO v3 are combined by the system to detect and locate risky pedestrians and the nearest vehicles. All identified information can provide the situational awareness to autonomous vehicles or human drivers for identifying crash risks from the surrounding traffic. To address the scarcity of annotated image datasets for studying traffic crashes, two completely new datasets have been developed by this paper and made available to the public, which were proved to be effective in training the proposed deep neural networks. The paper further evaluates the performance of the Multi-Net and the efficiency of the developed system. Comprehensive scene analysis is further illustrated with representative examples. Results demonstrate the effectiveness of the developed system and datasets for driving scene analysis, and their supportiveness for crash risk assessment and crash prevention. <a href='https://github.com/monjurulkarim/vehicle_distance'><b> Project Link </b></a></p>
                            <div class="subsubheading mb-0"> <b>This resarch is supported by MATC University Transportation Center <a href='http://matc.unl.edu/'> (http://matc.unl.edu/) </a> </b> </div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">December 2011 - March 2013</span></div> -->
                    </div>

             <hr>

                    <h2 class="mb-5">Other Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A Driving Simulator Based Study for Evaluating Safe Development of Autonomous Truck Mounted Attenuators Vehicle </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\simulator.PNG" alt="simulator" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b>Developed a driving simulation using blender gaming engine to collect data from drivers to better understand the impact of employing ATMA ( Autonomous Truck Mounted Attenuator) </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by <a href='https://isc.mst.edu/'>  Intelligent Systems Center at Missouri University of Science and Technology </a></b></div>
                            <div class="subheading mb-5"></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                 <hr>


                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Object detection and tracking using Mask RCNN and temporal coherence </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\manufactur.gif" alt="simulator" width="512" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b> This is the implementation of manufacturing Object detection and tracking in the manufacturing plants. This model uses Mask RCNN model to do the initial segmentation. Which is based on Feature Pyramid Network(FPN) and a ResNet50 backbone. To give temporal consistency in the detection results, a two-staged detection threshold has been used to boost up weak detections in a frame by referring to objects with high detection scores in neighboring frames. | <a href='https://github.com/monjurulkarim/Tracking_manufacturing'><b> Project </b></a> </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />

            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <div class="subheading mb-3">Journal Papers</div>
                    <p> 1. <b>Karim, M.M. </b>, Qin, R., Wang, Y. (2024). Fusion-GRU: A Deep Learning Model for Future Bounding Box Prediction of Traffic Agents in Risky Driving Videos,</i> Transportation Research Record </i>, <a href='https://journals.sagepub.com/doi/abs/10.1177/03611981241230540'> DOI </a></p>
                    <p> 2. <b>Karim, M.M. </b>, Qin, R., Yin, Z. (2023). An Attention-guided Multistream Feature Fusion Network for Localization of Risky Objects in Driving Videos, <i>IEEE Transaction on Intelligent Vehicles</i>, doi: 10.1109/TIV.2023.3275543. <a href='https://ieeexplore.ieee.org/abstract/document/10123114'> Preprint </a>| <a href='https://github.com/monjurulkarim/ROL_Dataset'> Code </a> </p>
                    <p> 3. <b>Karim, M.M. </b>,Li, Y., Qin, R., Yin, Z. (2022). A dynamic spatial-temporal attention network for early anticipation of traffic accidents. <i>IEEE Transaction on Intelligent Transportation Systems </i>vol. 23, no. 7, pp. 9590-9600. <a href="https://arxiv.org/abs/2106.10197"> Preprint </a>| <a href='https://github.com/monjurulkarim/DSTA'> Code </a> </p>
                    <p> 4. <b>Karim, M.M.</b>, Li, Y., Qin, R.(2022). Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. <i>Transportation Research Record,</i> 2676(6), 743-755. <a href='https://arxiv.org/abs/2108.00273'> Preprint </a>| <a href='https://github.com/monjurulkarim/xai-accident'> Code </a> </p>
                    <p> 5. <b>Karim, M.M. </b>, Qin, R., Yin, Z., & Chen, G. (2021). A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. <i> Structural Health Monitoring,</i> 21(3), 835-852. <a href="https://drive.google.com/file/d/1hemZZRymYFOfXP1g_MJQbItz48SQhhpu/view"> Preprint </a> | <a href='https://github.com/monjurulkarim/active_learning'> Code </a> </p>

                    <p> 6. Li, Y., <b>Karim, M.M.</b>, Qin, R., Sun, Z., Wang, Z., Yin, Z. (2021). Crash report data analysis for creating scenario-sise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks. <i>Accident Analysis and Prevention.</i>,151, pp. 105962. <a href="https://www.sciencedirect.com/science/article/pii/S0001457520317826"> Preprint </a>  </p>

                    <div class="subheading mb-3">Peer-Reviewed Conference Papers</div>
                    <p> 1. <b>Karim, M.M.</b>, Li, Y., Qin, R., Yin, Z. (2021). A system of vision sensor based deep neural networks for complex driving scene analysis in support of crashrisk assessment and prevention, <i>The 100th Transportation Research Board(TRB) Annual Meeting,</i> Virtual Meeting, January 5-29,2021. <a href='https://arxiv.org/abs/2106.10319'> Preprint </a> | <a href='https://github.com/monjurulkarim/vehicle_distance'> Code </a> </p>
                    <p> 2. <b>Karim, M.M.</b>, Dagli, CH. (2020). Sos meta-architecture selection for infrastructure inspection system using aerial drones. <i>In Proceeding of the 15th IEEE International Symposium on System of Systems Engineering (SoSE 2020).</i> Budapest, Hungary. June 2-4, 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9130538"> DOI </a> </p>
                    <p> 3. <b>Karim, M.M.</b>, Dagli, CH., & Qin, R. (2019). Modeling and simulation of a robotic bridge inspection system. <i>In Proceedings of the 2019 Complex Adaptive Systems Conference (CAS’19).</i> Malvern, PA. November 13-15, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S1877050920304154"> DOI </a> </p>
                    <p> 4. <b>Karim, M.M.</b>, Doell, D., Lingard, R., Yin, Z., Leu, MC., & Qin, R. (2019). A region-based deep learning algorithm for detecting and tracking objects in manufacturing plants. <i>In Proceedings of the 25th International Conference on Production Research (ICPR’19).</i> Chicago, IL. August 9-14, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S235197892030353X"> DOI </a> | <a href='https://github.com/monjurulkarim/Tracking_manufacturing'> Code </a> </p>
                    <!-- <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p> -->
                    <div class="subheading mb-3">Technical Reports</div>
                    <p> 1. Qin, R.,  Chen, G., Long, S.K., Yin, Z., Louis, S. <b> Karim, M.M. </b>, Zhao, T., (2020). A training framework of robotic operation and image analysis for decision-making in bridge inspection and preservation (Technical Report INSPIRE-006). USDOT INSPIRE University Transportation Center. <a href="https://inspire-utc.mst.edu/researchprojects/wd-1/"> Website </a> </p>
                    <p> 2. Qin, R., Yin, Z., <b> Karim, M.M. </b>, Li, Y., Wang, Z. (2020). Crash prediction and avoidance by identifying and evaluating risk factors from onboard cameras (Technical Report 25-1121-0005-135-2). USDOT MATC University Transportation Center. <a href="http://matc.unl.edu/research/research_project.php?researchID=560"> Download </a> </p>
                </div>
            </section>
            <hr class="m-0" />


            <!-- Teaching -->
<section class="resume-section" id="Teaching">
    <div class="resume-section-content">
        <h2 class="mb-5">Teaching</h2>

        <div class="d-flex flex-column flex-md-row justify-content-between mb-4">
            <div class="flex-grow-1">
                <h3 class="mb-0">Co-Instructor</h3>
                <div class="subheading mb-3">Course : AI 101: Introduction to Artificial Intelligence (AI) </div>
                <div>This introductory short course, organized by the PacTrans Workforce Development Institute, is specifically designed for transportation professionals. It was previously delivered to employees of the Washington State Department of Transportation (WSDOT) and covers the fundamentals of artificial intelligence (AI), including machine learning and deep learning. I co-taught this course with Dr. Yinhai Wang.
                  <p></p>  The next offering of this course is scheduled for April 29, 2025, and enrollment is now open. You can  - <a href="https://lnkd.in/gw24k2e5"> register here </a></p>
                </div>
              </div>
            <div class="flex-shrink-0"><span class="text-primary">Spring 2025</span></div>
            </div>

        <div class="d-flex flex-column flex-md-row justify-content-between mb-4">
        <div class="flex-grow-1">
            <h3 class="mb-0">Instructor</h3>
            <div class="subheading mb-3">Course : CET 590 (Traffic Systems Operations) </div>
            <div>CET 590 is a graduate course that is offered from the Department of Civil and Environmental Engineering at the University of Washington. As the instructor of this course I taught traffic control system concepts, components, and algorithms. Major topics of this course include, traffic control systems, timing plan design, traffic flow characteristics, driver behavior modeling, vehicle actuated programming, and simulation. Vissim traffic simulation package is also taught in this course to evaluate the performance of traffic operation plans.
            </div>
          </div>
        <div class="flex-shrink-0"><span class="text-primary">Fall 2023</span></div>
        </div>

        <p>&nbsp;</p>

      <div class="d-flex flex-column flex-md-row justify-content-between mb-4">
        <div class="flex-grow-1">
            <h3 class="mb-0">Co-Instructor</h3>
            <div class="subheading mb-3">Course : CIV 555 (Analytics for Engineering Systems) </div>
            <div>CIV 555 is a graduate course that is offered from the Civil Engineering Department at Stony Brook University. I was a co-instructor for this course with Dr. Ruwen Qin in this Fall 2022 and Fall 2021 semester. I taught Neural Networks and Deep Learning.
            </div>
          </div>
        <div class="flex-shrink-0"><span class="text-primary">Fall 2021, Fall 2022</span></div>
        </div>

<!--   <ul class="fa-ul mb-0">

          <div class="subheading mb-3">Lectures and Notes</div>
              <li>
                  <b>Lecture 11</b>: Introduction to Neural Network [<a href="https://bit.ly/3fwF54L" target='_blank'> Slides </a>]
              </li>

              <li>
                  <b> Lecture 12</b>: Neural Network with Python [<a href="https://bit.ly/3IgfPfI" target='_blank'> Slides </a> | <a href="https://drive.google.com/file/d/1mSguYtFlEYhpERdlhH3CLTMjepiTRS4U/view?usp=sharingPython" target='_blank'> Python Notebook </a>]
              </li>
              <li>
                  <b> Lecture 13</b>: Backpropagation and Gradient Descent [<a href="https://bit.ly/3KlGPfy" target='_blank'> Slides </a> | <a href="https://drive.google.com/file/d/1Nj-T1_qXpqSuxQfx6Epc4E4YY55dlLnP/view?usp=sharing" target='_blank'> Homework </a>]
              </li>
              <li>
                  <b> Lecture 21</b>: Deep Learning [<a href="https://bit.ly/3A7k0Yc" target='_blank'> Slides </a>]
              </li>
              <li>
                  <b> Lecture 22</b>: Deep Learning Continued [<a href="https://docs.google.com/presentation/d/15BBaCRE5aouve5TaI5qstaTVVqVGm_De/edit?usp=sharing&ouid=108305137043658522562&rtpof=true&sd=true" target='_blank'> Slides </a> | <a href="https://drive.google.com/drive/folders/1qTbT2Am3O7BALyc0tda4Z6O7g9RAVoZc?usp=sharing" target='_blank'> Python Notebook </a> | <a href="https://drive.google.com/file/d/1fLIQkKnUgU-BP5D6DvI7xxx-2GyxCvuw/view?usp=sharing" target='_blank'> Homework </a>]
              </li>
          </ul>  -->

        <p>&nbsp;</p>


        <div class="d-flex flex-column flex-md-row justify-content-between mb-4">
        <div class="flex-grow-1">
            <h3 class="mb-0">Guest Lecturer</h3>
            <div class="subheading mb-3">Course : CIV 355 (Data Analytics for Civil Engineering Systems) </div>
            <div>CIV 355 is an undergraduate course that is offered from the Civil Engineering Department at Stony Brook University. I delivered a lecture to introduce neural networks to undergraduate students.
            </div>
          </div>
        <div class="flex-shrink-0"><span class="text-primary">Spring 2022</span></div>

      </div>
    </div>
</section>
            <hr class="m-0" />




            <!-- Invited talk-->
            <!-- <section class="resume-section" id="talk"> -->
                <!-- <div class="resume-section-content"> -->
                    <!-- <h2 class="mb-5">Presentations (Invited)</h2> -->
                    <!-- <p>September 22, 2020 | 2020 System of Systems Engineering Collaborators Information Exchange (SoSECIE) Webinar organized by MITRE | <a href='https://www.youtube.com/watch?v=XPKOloDWhwE&feature=youtu.be'>  Recording </a></p> -->
                    <!-- </div> -->
            <!-- </section> -->
            <!-- <hr class="m-0" /> -->


            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                          <ul class="fa-ul mb-0">
                            <li>
                              <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                             2025 Young Scholar Award from TRB AED30 (Information Systems and Technology Committee)
                            </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2025</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                          <ul class="fa-ul mb-0">
                            <li>
                              <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                             2022 CIV Research Merit Award, Stony Brook University
                            </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2022</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                          <ul class="fa-ul mb-0">
                            <li>
                              <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                             Honorable mention – Graduate Student Research Symposium Competition organized by Department of CE, Stony Brook University
                            </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2022</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                        <div class="flex-grow-1">
                          <ul class="fa-ul mb-0">
                            <li>
                              <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                              Finalist - Student Competition organized by ASCE T&DI Technical Committee on Artificial Intelligence
                            </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2021</span></div>
                    </div>



                      <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                          <div class="flex-grow-1">
                            <ul class="fa-ul mb-0">
                              <li>
                                <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                                2
                                <sup>nd</sup>
                                Place - INSPIRE UTC Graduate Student Poster Competetion 2020
                              </li>
                            </ul>
                          </div>
                          <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                      </div>

                      <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                          <div class="flex-grow-1">
                            <ul class="fa-ul mb-0">
                              <li>
                                <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                                2
                                <sup>nd</sup>
                                Place - Intelligent Systems Center Poster Competion - Missouri University of Science & Technology 2019
                              </li>
                            </ul>
                          </div>
                          <div class="flex-shrink-0"><span class="text-primary">2019</span></div>
                      </div>

                      <div class="d-flex flex-column flex-md-row justify-content-between mb-2">
                          <div class="flex-grow-1">
                            <ul class="fa-ul mb-0">
                              <li>
                                <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                                2
                                <sup>nd</sup>
                                Place - Best Paper Award - Complex Adaptive Systems Conference 2019
                              </li>
                            </ul>
                          </div>
                          <div class="flex-shrink-0"><span class="text-primary">2019</span></div>
                      </div>
                </div>
            </section>
            </section>
            <hr class="m-0" />


            <!-- Blog-->
            <section class="resume-section" id="blog">
                <div class="resume-section-content">
                    <h2 class="mb-5">Blog</h2>
                    <ul class="fa-ul mb-0">

                        <li>
                     <div class="leftcolumn">
                          <div class="card">
                            <h3><a href="https://medium.com/@raju.monjurulkarim/i-built-an-self-hosted-ai-meeting-note-taker-that-runs-100-offline-heres-how-you-can-too-d110b7ef0b95" target="_blank">I Built a self-hosted AI Meeting Note Taker That Runs 100% Offline — Here’s How You Can Too </a></h3>
                            <!-- <h5>A step-by-step guide to developing explainable AI to classify surface crack images with Pytorch. This is part 1 of 2 part series.</h5> -->
                            <p>By <a href="https://medium.com/@raju.monjurulkarim" target="_blank">Muhammad Monjurul Karim </a></p>
                            <img src="assets\img\ai-note.PNG" alt="crack" width="512" height="250">
                            <p> Let’s build a private, unlimited, and self-hosted AI-note taker with Python, Whisper, and Ollama..</p>
                            <p><a href="https://medium.com/@raju.monjurulkarim/i-built-an-self-hosted-ai-meeting-note-taker-that-runs-100-offline-heres-how-you-can-too-d110b7ef0b95" target="_blank">See more ... </a></p>
                          </div>
                        </div>
                        </li>
                        
                        <li>
                          <div class="leftcolumn">
                          <div class="card">
                            <h3><a href="https://towardsdatascience.com/tutorial-on-surface-crack-classification-with-visual-explanation-part-1-14542d2ea7ac" target="_blank">Tutorial on surface crack classification with visual explanation (Part 1)</a></h3>
                            <!-- <h5>A step-by-step guide to developing explainable AI to classify surface crack images with Pytorch. This is part 1 of 2 part series.</h5> -->
                            <p>By <a href="https://medium.com/@raju.monjurulkarim" target="_blank">Muhammad Monjurul Karim </a></p>
                            <img src="assets\img\crack.PNG" alt="crack" width="512" height="250">
                            <p> A step-by-step guide to develop explainable AI to classify surface crack images with Pytorch. This is part 1 of 2 part series.</p>
                            <p><a href="https://towardsdatascience.com/tutorial-on-surface-crack-classification-with-visual-explanation-part-1-14542d2ea7ac" target="_blank">See more ... </a></p>
                          </div>
                        </div>
                        </li>
                      <li>
                          <div class="leftcolumn">
                          <div class="card">
                            <h3><a href="https://medium.com/@raju.monjurulkarim/tutorial-on-surface-crack-classification-with-visual-explanation-part-2-f8638960d0d7" target="_blank">Tutorial on surface crack classification with visual explanation (Part 2)</a></h3>
                            <!-- <h5>A step-by-step guide to developing explainable AI to classify surface crack images with Pytorch. This is second part of 2 part series.</h5> -->
                            <p>By <a href="https://medium.com/@raju.monjurulkarim" target="_blank">Muhammad Monjurul Karim </a></p>
                            <img src="assets\img\for_xai_blog.PNG" alt="crack" width="512" height="250">
                            <p> A step-by-step guide to developing explainable AI to classify surface crack images with Pytorch. This is second part of 2 part series.</p>
                            <p><a href="https://medium.com/@raju.monjurulkarim/tutorial-on-surface-crack-classification-with-visual-explanation-part-2-f8638960d0d7" target="_blank">See more ... </a></p>
                          </div>
                        </div>
                        </li>
                    </ul>
                </div>
            </section>

                <!-- Contact-->
            <section class="resume-section" id="contact">
              <div class="resume-section-content">
                  <h2 class="mb-5">Contact Me</h2>
<p>Let's connect! Whether you're interested in a research collaboration, need expert consultation, or have a project idea you'd like to discuss, I'm eager to explore opportunities. Send me a message below.</p>
                  <form action="https://api.web3forms.com/submit" method="POST" id="contactForm">
                      <input type="hidden" name="access_key" value="d136f972-2640-4798-9d13-7f75e27fb57b">
                      <div class="form-group">
                          <label for="name">Name:</label>
                          <input type="text" class="form-control" id="name" name="name" required>
                      </div>
                      <div class="form-group">
                          <label for="email">Email:</label>
                          <input type="email" class="form-control" id="email" name="email" required>
                      </div>
                      <div class="form-group">
                          <label for="message">Message:</label>
                          <textarea class="form-control" id="message" name="message" rows="5" required></textarea>
                      </div>
                      <button type="submit" class="btn btn-primary">Send Message</button>
                      <div id="form-messages" class="mt-3"></div>
                  </form>
              </div>
          </section>
          <hr class="m-0" />
      </div>



            <!-- Other-->
            <section class="other-section" id="others">
                <div class="sr-only", style="width:512px;height:224px>
                    <ul class="fa-ul mb-0">
                    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vd3ZRyx86nwX6pEXvK45MTWhDTJg0sx2mbg3BwhcQak&cl=ffffff&w=a",  width="512" height="224"></script>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>

         <!-- Script for form handling-->
        <script>
          document.getElementById("contactForm").addEventListener("submit", function(event) {
            event.preventDefault();

            const form = event.target;
            const formData = new FormData(form);

            fetch(form.action, {
                method: form.method,
                body: formData,
                headers: {
                    'Accept': 'application/json'
                }
            })
            .then(response => {
                const formMessages = document.getElementById("form-messages");
                if (response.ok) {
                    // Success
                    formMessages.innerHTML = '<div class="alert alert-success">Thank you for your message!</div>';
                    form.reset();
                } else {
                    // Error
                    response.json().then(data => {
                        if (Object.hasOwn(data, 'errors')) {
                            formMessages.innerHTML = '<div class="alert alert-danger">' + data["errors"].map(error => error["message"]).join(", ") + '</div>';
                        } else {
                            formMessages.innerHTML = '<div class="alert alert-danger">Oops! There was a problem.</div>';
                        }
                    });
                }
            })
            .catch(error => {
                const formMessages = document.getElementById("form-messages");
                formMessages.innerHTML = '<div class="alert alert-danger">Oops! Network error. Please try again.</div>';
                console.error("Network error:", error);
            });
        });
    </script>
        </script>

    </body>
</html>
